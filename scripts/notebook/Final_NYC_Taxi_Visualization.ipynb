{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca89aa7b-68e0-4ca9-9401-ff0ca5863264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/11 23:58:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPricingDynamicsAnalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read the data from HDFS into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Select only the necessary columns and filter out any possible outliers or incorrect data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fares_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_miles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_passenger_fare\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfilter((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_miles\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_passenger_fare\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PricingDynamicsAnalysis\").getOrCreate()\n",
    "\n",
    "# Read the data from HDFS into a DataFrame\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\")\n",
    "\n",
    "# Select only the necessary columns and filter out any possible outliers or incorrect data\n",
    "fares_df = df.select(\"trip_miles\", \"base_passenger_fare\").filter((col(\"trip_miles\") > 0) & (col(\"base_passenger_fare\") > 0))\n",
    "\n",
    "bin_size = 10\n",
    "\n",
    "# Define the mile bins (You can adjust the range and bin size as needed)\n",
    "mile_bins = list(range(0, int(fares_df.agg({\"trip_miles\": \"max\"}).collect()[0][0]) + bin_size, bin_size))\n",
    "\n",
    "# Assign each trip to a bin based on trip_miles\n",
    "fares_df = fares_df.withColumn(\"mile_bin\", (col(\"trip_miles\") / bin_size).cast(\"integer\") * bin_size)\n",
    "\n",
    "# Group by the mile bins and calculate the average fare\n",
    "avg_fare_by_bin = fares_df.groupBy(\"mile_bin\").agg(avg(\"base_passenger_fare\").alias(\"avg_fare\")).orderBy(\"mile_bin\")\n",
    "\n",
    "# Collect the data to the driver for visualization\n",
    "avg_fare_by_bin_data = avg_fare_by_bin.toPandas()\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(avg_fare_by_bin_data['mile_bin'], avg_fare_by_bin_data['avg_fare'], '-o', color='turquoise', label='Average Fare by Miles')\n",
    "plt.fill_between(avg_fare_by_bin_data['mile_bin'], avg_fare_by_bin_data['avg_fare'], color='lightblue', alpha=0.3)  # For area plot\n",
    "\n",
    "# Annotate each point with its value\n",
    "for index, row in avg_fare_by_bin_data.iterrows():\n",
    "    plt.annotate(\n",
    "        f\"{row['avg_fare']:.2f}\",  # Format to 2 decimal places\n",
    "        (row['mile_bin'], row['avg_fare']),\n",
    "        textcoords=\"offset points\",  # how to position the text\n",
    "        xytext=(0, 10),  # distance from text to points (x, y)\n",
    "        ha=\"center\", \n",
    "        size=10,\n",
    "        color='black')  # horizontal alignment can be left, right or center\n",
    "\n",
    "# Fix x-axis overlap and set custom ticks\n",
    "plt.xticks(mile_bins, rotation=45)  # Rotate x-axis labels to prevent overlap\n",
    "plt.xlabel('Trip Distance Bin (Miles)')\n",
    "plt.ylabel('Average Base Passenger Fare ($)')\n",
    "plt.title('Average Base Passenger Fare by Trip Miles Bin ($)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to prevent cutting off labels\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75920d-9b38-4a61-9e29-93e8b5fa29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, max as spark_max, sum as spark_sum\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SharedMobilityAssessment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# Replace the path with your actual data path\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\")\n",
    "\n",
    "# Create a temporary view so you can use Spark SQL\n",
    "df.createOrReplaceTempView(\"fhv_trip_data\")\n",
    "\n",
    "# Shared Trip Proportion\n",
    "shared_trips_df = df.filter(col(\"shared_request_flag\") == 'Y')\n",
    "total_shared_trips = shared_trips_df.count()\n",
    "total_trips = df.count()\n",
    "shared_trip_proportion = (total_shared_trips / total_trips) * 100\n",
    "\n",
    "# Effectiveness of Shared Trips\n",
    "effective_shared_trips = shared_trips_df.filter(col(\"shared_match_flag\") == 'Y').count()\n",
    "shared_trip_effectiveness = (effective_shared_trips / total_shared_trips) * 100 if total_shared_trips != 0 else 0\n",
    "\n",
    "# Visualization\n",
    "# Shared Trip Proportion Pie Chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Pie chart for shared trips proportion\n",
    "labels = 'Shared Trips', 'Non-Shared Trips'\n",
    "sizes = [shared_trip_proportion, 100 - shared_trip_proportion]\n",
    "explode = (0.1, 0)  # explode the 1st slice for emphasis\n",
    "colors = ['#ff9999','#66b3ff']\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, colors=colors)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.set_title('Proportion of Shared Trips')\n",
    "\n",
    "# Bar chart for effectiveness of shared trips\n",
    "labels = ['Shared Trips']\n",
    "effectiveness = [shared_trip_effectiveness]\n",
    "\n",
    "ax2.bar(labels, effectiveness, color='#ff9999')\n",
    "ax2.set_title('Effectiveness of Shared Trips')\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# Annotate the bar chart with the percentage value\n",
    "for i in range(len(effectiveness)):\n",
    "    ax2.annotate(f'{effectiveness[i]:.2f}%', \n",
    "                 xy=(i, effectiveness[i]), \n",
    "                 xytext=(0, 3),  # 3 points vertical offset\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the metrics using Spark SQL\n",
    "shared_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUM(CASE WHEN shared_request_flag = 'Y' THEN 1 ELSE 0 END) AS TotalSharedRequests,\n",
    "        SUM(CASE WHEN shared_match_flag = 'Y' THEN 1 ELSE 0 END) AS TotalSharedMatches,\n",
    "        SUM(CASE WHEN shared_request_flag = 'Y' AND shared_match_flag = 'Y' THEN 1 ELSE 0 END) AS SuccessfulSharedTrips\n",
    "    FROM fhv_trip_data\n",
    "\"\"\")\n",
    "\n",
    "# Calculate the proportion of matches that resulted in a shared journey\n",
    "proportion_of_successful_shared_trips = shared_metrics.select(\n",
    "    (col(\"SuccessfulSharedTrips\") / col(\"TotalSharedMatches\")).alias(\"ProportionOfSuccessfulSharedTrips\")\n",
    ")\n",
    "\n",
    "shared_metrics_pd = shared_metrics.toPandas()\n",
    "proportion_pd = proportion_of_successful_shared_trips.toPandas()\n",
    "\n",
    "# Plotting the bar chart for raw counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_labels = ['Total Shared Requests', 'Total Shared Matches', 'Successful Shared Trips']\n",
    "bar_values = shared_metrics_pd.iloc[0, :].tolist()\n",
    "plt.bar(bar_labels, bar_values, color=['blue', 'orange', 'green'])\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Shared Mobility Metrics')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Adding the text labels on the bars\n",
    "for i in range(len(bar_labels)):\n",
    "    plt.text(i, bar_values[i] + max(bar_values) * 0.01, str(bar_values[i]), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting the pie chart for the proportion of successful shared trips\n",
    "plt.figure(figsize=(8, 8))\n",
    "pie_labels = 'Unsuccessful Shared Trips', 'Successful Shared Trips'\n",
    "pie_values = [1 - proportion_pd.iloc[0, 0], proportion_pd.iloc[0, 0]]\n",
    "plt.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', startangle=140, colors=['red', 'green'])\n",
    "plt.title('Proportion of Successful Shared Trips')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c4e9b-b808-48d8-8d32-60b221a04466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ShortLongTripsAnalysis\").getOrCreate()\n",
    "\n",
    "# Read the data from HDFS into a DataFrame\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\")\n",
    "\n",
    "# Categorize trips into short or long based on the 30-mile criterion\n",
    "df_with_trip_type = df.withColumn(\n",
    "    \"trip_type\",\n",
    "    when(col(\"trip_miles\") <= 30, \"Short Trip (<= 30 miles)\").otherwise(\"Long Trip (> 30 miles)\")\n",
    ")\n",
    "\n",
    "# Count the number of short and long trips\n",
    "trip_counts = df_with_trip_type.groupBy(\"trip_type\").count()\n",
    "\n",
    "# Collect the data to the driver for visualization\n",
    "trip_counts_local = trip_counts.toPandas()\n",
    "\n",
    "# Plot the distribution of short and long trips\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(trip_counts_local['trip_type'], trip_counts_local['count'], color=['blue', 'orange'])\n",
    "plt.xlabel('Trip Type')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.title('Distribution of Short and Long Trips')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Adding the text labels on the bars\n",
    "for i in range(len(trip_counts_local['trip_type'])):\n",
    "    plt.text(i, trip_counts_local['count'][i] + max(trip_counts_local['count']) * 0.01, str(trip_counts_local['count'][i]), ha='center')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eb561-0738-4fa8-a9c0-507605adfb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLonHDFSData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the data from HDFS into a DataFrame\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\")\n",
    "\n",
    "# Create a temporary view from the DataFrame\n",
    "df.createOrReplaceTempView(\"merged_data\")\n",
    "\n",
    "# Analyze the distribution of trip distances\n",
    "df_dist = spark.sql('SELECT trip_miles FROM merged_data')\n",
    "\n",
    "# Describe the statistics of trip distances\n",
    "df_dist.describe().show()\n",
    "\n",
    "# Histogram of trip distances\n",
    "trip_miles_hist = df_dist.select('trip_miles').rdd.flatMap(lambda x: x).histogram(20)\n",
    "\n",
    "# Load the computed histogram into a Pandas DataFrame for plotting\n",
    "hist_df = pd.DataFrame(\n",
    "    list(zip(*trip_miles_hist)), \n",
    "    columns=['bin', 'frequency']\n",
    ")\n",
    "\n",
    "# Format the bin values to have only two decimal points\n",
    "hist_df['bin'] = hist_df['bin'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Reset the index to use integers for plotting, this will make the x-ticks labels clear and horizontal\n",
    "hist_df.reset_index(inplace=True)\n",
    "\n",
    "# Plotting\n",
    "ax = hist_df.plot(\n",
    "    x='index', y='frequency', kind='bar', figsize=(15, 5), legend=False\n",
    ")\n",
    "\n",
    "# Set the x-axis labels to the formatted bin values\n",
    "ax.set_xticklabels(hist_df['bin'], rotation=0)  # Set rotation to 0 for horizontal labels\n",
    "\n",
    "# Set the y-scale to logarithmic\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Trip Distance (miles)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Trip Distance')\n",
    "\n",
    "# Annotate each bar with the bin range and frequency\n",
    "for idx, row in hist_df.iterrows():\n",
    "    ax.text(idx, row['frequency'], f\"{row['frequency']}\", ha='center', va='bottom')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a22eb-0bac-4caa-809b-ad416ebf5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLonParquetHDFSData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the data from HDFS into a DataFrame (from Parquet file)\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/tmp/hadoop-yuvrajpatadia/dfs/data/merged.parquet\")\n",
    "\n",
    "# Create a temporary view from the DataFrame\n",
    "df.createOrReplaceTempView(\"merged_data\")\n",
    "\n",
    "# Execute the SQL queries for Pickup and Dropoff counts\n",
    "df_pu = spark.sql('''\n",
    "    SELECT PULocationID AS LocationID, count(*) AS PUcount\n",
    "    FROM merged_data\n",
    "    GROUP BY PULocationID\n",
    "''')\n",
    "df_do = spark.sql('''\n",
    "    SELECT DOLocationID AS LocationID, count(*) AS DOcount\n",
    "    FROM merged_data\n",
    "    GROUP BY DOLocationID\n",
    "''')\n",
    "\n",
    "# Perform an outer join on LocationID\n",
    "df_q = df_pu.join(df_do, \"LocationID\", \"outer\")\n",
    "\n",
    "# Find the top 50 pickup and dropoff locations\n",
    "PUtop = df_q.orderBy(col(\"PUcount\").desc())\n",
    "DOtop = df_q.orderBy(col(\"DOcount\").desc())\n",
    "\n",
    "# Collect data to the driver node for plotting\n",
    "PUtop_local = PUtop.toPandas()\n",
    "DOtop_local = DOtop.toPandas()\n",
    "\n",
    "# Add a new column with the hour of pickup and dropoff, then group by this new column\n",
    "df_pu_spark = spark.sql('''\n",
    "    SELECT hour(pickup_datetime) AS hour, count(*) AS PUcount\n",
    "    FROM merged_data\n",
    "    GROUP BY hour(pickup_datetime)\n",
    "''')\n",
    "df_do_spark = spark.sql('''\n",
    "    SELECT hour(dropoff_datetime) AS hour, count(*) AS DOcount\n",
    "    FROM merged_data\n",
    "    GROUP BY hour(dropoff_datetime)\n",
    "''')\n",
    "\n",
    "# Perform a join operation on the 'hour' column\n",
    "df_q2_spark = df_pu_spark.join(df_do_spark, \"hour\", \"outer\").fillna(0)\n",
    "\n",
    "# Sort the values by 'hour'\n",
    "df_q2_spark = df_q2_spark.orderBy(\"hour\")\n",
    "\n",
    "# Collect data to the driver node for plotting\n",
    "df_q2_pandas = df_q2_spark.toPandas()\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df_q2_pandas['hour'], df_q2_pandas['PUcount'], '-o', label='Pick-ups')\n",
    "plt.plot(df_q2_pandas['hour'], df_q2_pandas['DOcount'], '-o', label='Drop-offs')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pick-up and Drop-off Counts by Hour')\n",
    "plt.xticks(range(24))  # Set x-axis ticks to show every hour\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978f001-4f99-4322-a890-7dff53cb58bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/11 23:58:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Polygon as PolygonPatch\n",
    "import shapefile\n",
    "import numpy as np\n",
    "import shapefile\n",
    "import pandas as pd\n",
    "\n",
    "def get_boundaries(sf):\n",
    "    lat, lon = [], []\n",
    "    for shape in list(sf.iterShapes()):\n",
    "        lat.extend([shape.bbox[0], shape.bbox[2]])\n",
    "        lon.extend([shape.bbox[1], shape.bbox[3]])\n",
    "\n",
    "    margin = 0.01 # buffer to add to the range\n",
    "    lat_min = min(lat) - margin\n",
    "    lat_max = max(lat) + margin\n",
    "    lon_min = min(lon) - margin\n",
    "    lon_max = max(lon) + margin\n",
    "\n",
    "    return lat_min, lat_max, lon_min, lon_max\n",
    "\n",
    "def get_lat_lon(sf):\n",
    "    content = []\n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        loc_id = rec[shp_dic['LocationID']]\n",
    "        \n",
    "        x = (shape.bbox[0]+shape.bbox[2])/2\n",
    "        y = (shape.bbox[1]+shape.bbox[3])/2\n",
    "        \n",
    "        content.append((loc_id, x, y))\n",
    "    return pd.DataFrame(content, columns=[\"LocationID\", \"longitude\", \"latitude\"])\n",
    "\n",
    "def draw_zone_map(ax, sf, heat={}, text=[], arrows=[]):\n",
    "    continent = [235/256, 151/256, 78/256]\n",
    "    ocean = (89/256, 171/256, 227/256)\n",
    "    theta = np.linspace(0, 2*np.pi, len(text)+1).tolist()\n",
    "    ax.set_facecolor(ocean)\n",
    "    \n",
    "    # colorbar\n",
    "    if len(heat) != 0:\n",
    "        heat_values = list(heat.values())  # Make sure heat.values() is a flat list of numbers\n",
    "        vmin = min(heat_values)\n",
    "        vmax = max(heat_values)\n",
    "        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        cm = plt.get_cmap('Reds')\n",
    "        sm = mpl.cm.ScalarMappable(cmap=cm, norm=norm)\n",
    "        sm.set_array([])\n",
    "        # Use 'fig.colorbar' instead of 'plt.colorbar'\n",
    "        cbar = fig.colorbar(sm, ax=ax, ticks=np.linspace(vmin, vmax, 8), \n",
    "                            boundaries=np.arange(vmin - 10, vmax + 10, .1))\n",
    "        cbar.ax.set_yticklabels([f'{int(tick)}' for tick in cbar.get_ticks()])  # Set the colorbar labels as integers\n",
    "\n",
    "    # Proceed with plotting shapes and annotations\n",
    "    # ... (rest of the function remains the same)\n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        loc_id = rec[shp_dic['LocationID']]  # Update with correct index or field name\n",
    "        zone = rec[shp_dic['zone']]  # Update with correct index or field name\n",
    "        \n",
    "        if len(heat) == 0:\n",
    "            col = continent\n",
    "        else:\n",
    "            if loc_id not in heat:\n",
    "                R, G, B, A = cm(norm(0))\n",
    "            else:\n",
    "                R, G, B, A = cm(norm(heat[loc_id]))\n",
    "            col = [R, G, B]\n",
    "\n",
    "        # check number of parts (could use MultiPolygon class of shapely?)\n",
    "        nparts = len(shape.parts) # total parts\n",
    "        if nparts == 1:\n",
    "            polygon = PolygonPatch(shape.points, facecolor=col, alpha=1.0, zorder=2)\n",
    "            ax.add_patch(polygon)\n",
    "        else: # loop over parts of each shape, plot separately\n",
    "            for ip in range(nparts): # loop over parts, plot separately\n",
    "                i0 = shape.parts[ip]\n",
    "                if ip < nparts - 1:\n",
    "                    i1 = shape.parts[ip + 1] - 1\n",
    "                else:\n",
    "                    i1 = len(shape.points)\n",
    "\n",
    "                polygon = PolygonPatch(shape.points[i0:i1+1], facecolor=col, alpha=1.0, zorder=2)\n",
    "                ax.add_patch(polygon)\n",
    "        \n",
    "        x = (shape.bbox[0] + shape.bbox[2]) / 2\n",
    "        y = (shape.bbox[1] + shape.bbox[3]) / 2\n",
    "        if len(text) == 0:\n",
    "            plt.text(x, y, str(loc_id), horizontalalignment='center', verticalalignment='center')\n",
    "        elif len(text) != 0 and loc_id in text:\n",
    "            eta_x = 0.05 * np.cos(theta[text.index(loc_id)])\n",
    "            eta_y = 0.05 * np.sin(theta[text.index(loc_id)])\n",
    "            ax.annotate(\"[{}] {}\".format(loc_id, zone), xy=(x, y), xytext=(x+eta_x, y+eta_y),\n",
    "                        bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12,\n",
    "                        arrowprops=dict(facecolor='black', width=3, shrink=0.05))\n",
    "    if len(arrows) != 0:\n",
    "        for arr in arrows:\n",
    "            ax.annotate('', xy=arr['dest'], xytext=arr['src'], size=arr['cnt'],\n",
    "                        arrowprops=dict(arrowstyle=\"fancy\", fc=\"0.6\", ec=\"none\"))\n",
    "    \n",
    "    # display\n",
    "    limits = get_boundaries(sf)\n",
    "    plt.xlim(limits[0], limits[1])\n",
    "    plt.ylim(limits[2], limits[3])\n",
    "\n",
    "\n",
    "sf = shapefile.Reader(\"taxi_zones.shp\")\n",
    "fields_name = [field[0] for field in sf.fields[1:]]\n",
    "\n",
    "shp_dic = dict(zip(fields_name, list(range(len(fields_name)))))\n",
    "\n",
    "attributes = sf.records()\n",
    "shp_attr = [dict(zip(fields_name, attr)) for attr in attributes]\n",
    "\n",
    "df_loc = pd.DataFrame(shp_attr).join(get_lat_lon(sf).set_index(\"LocationID\"), on=\"LocationID\")\n",
    "df_loc.head()\n",
    "\n",
    "# Convert the all DataFrames to dictionaries mapping LocationID to PUcount and DOcount\n",
    "PUtop_dict = dict(zip(PUtop_local['LocationID'], PUtop_local['PUcount']))\n",
    "DOtop_dict = dict(zip(DOtop_local['LocationID'], DOtop_local['DOcount']))\n",
    "\n",
    "# Load the shapefile\n",
    "sf = shapefile.Reader(\"taxi_zones.shp\")\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "\n",
    "# Plot for pickups\n",
    "axes[0].set_title(\"Zones with most pickups\")\n",
    "draw_zone_map(axes[0], sf, heat=PUtop_dict, text=[str(loc_id) for loc_id in PUtop_dict.keys()])\n",
    "\n",
    "# Plot for drop-offs\n",
    "axes[1].set_title(\"Zones with most drop-offs\")\n",
    "draw_zone_map(axes[1], sf, heat=DOtop_dict, text=[str(loc_id) for loc_id in DOtop_dict.keys()])\n",
    "\n",
    "# Set boundaries for the axes based on the shapefile boundaries\n",
    "boundaries = get_boundaries(sf)\n",
    "for ax in axes:\n",
    "    ax.set_xlim([boundaries[0], boundaries[1]])\n",
    "    ax.set_ylim([boundaries[2], boundaries[3]])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
